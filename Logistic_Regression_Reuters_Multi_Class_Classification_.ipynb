{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression: Reuters Multi-Class Classification .ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mfavaits/YouTube-Series-on-Machine-Learning/blob/master/Logistic_Regression_Reuters_Multi_Class_Classification_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Aoe_p0AaYhO",
        "colab_type": "text"
      },
      "source": [
        "The Reuters dataset, a set of short newswires and their topics, published by Reuters in 1986. Itâ€™s a simple, widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each topic has at least 10 examples in the training set. The set contain 8982 training samples and 2246 test samples\n",
        "Like IMDB the Reuters dataset comes packaged as part of Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de3MGD6O72TY",
        "colab_type": "code",
        "outputId": "772e3ccb-c6bf-470f-a697-a7d5249950e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.datasets import reuters"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ptvpk_utmPjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary=7500 # we will use on the first 7500 most used words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCO-EHN9EhPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save np.load\n",
        "np_load_old = np.load\n",
        "\n",
        "# modify the default parameters of np.load\n",
        "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
        "\n",
        "# call load_data with allow_pickle implicitly set to true\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=vocabulary)\n",
        "\n",
        "# restore np.load for future normal usage\n",
        "np.load = np_load_old"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh2qyX4pAaqe",
        "colab_type": "text"
      },
      "source": [
        "Next code block is to fix a bug in keras just as we did with IMDB dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNnyWLGSmtLA",
        "colab_type": "code",
        "outputId": "f5ec17d5-03a5-4eff-8a05-5e64a606ce50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8982"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E2hc3GnmyQM",
        "colab_type": "code",
        "outputId": "ad51d8bb-1cba-4f03-d29e-a021c177bd56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(test_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2246"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG560c2_CYN6",
        "colab_type": "text"
      },
      "source": [
        "In the next line of code we will print the lists that contain sequences of words represented by a word index. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5qZFmD4Wx6l",
        "colab_type": "code",
        "outputId": "9ac6cd87-a97f-4f9a-dd95-10eea85d08aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "print(train_data[1]) # train_data is a list of word sequences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 3267, 699, 3434, 2295, 56, 2, 2, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 2, 49, 2295, 2, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 2, 2, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwgfRMAj2Hip",
        "colab_type": "text"
      },
      "source": [
        "Now we will vectorize the training and test data. Basically we will create a matrix where the rows are the reviews and where the columns represent the vocabulary (7500 columns). We will set a 1 in the correct column if the word of the review matches a word of the vocabulary. As we limit the reviews to 150 words there will be 7350 places where we will have a zero. This means that matrix will be rather sparse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QELAzmk-03TM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_sequences(sequences, dimension=vocabulary):\n",
        "    results=np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence]=1\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRbl4N4d13te",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train=vectorize_sequences(train_data)\n",
        "x_test=vectorize_sequences(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJzq9rVAnhsg",
        "colab_type": "text"
      },
      "source": [
        "To vectorize the labels we will use one-hot encoding. One-hot encoding is a widely used format for categorical data, also. In this case, one-hot encoding of the labels consists of embedding each label as an all-zero vector with a 1 in the place of the label index. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkC9VSvEnxgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "one_hot_train_labels = to_categorical(train_labels)\n",
        "one_hot_test_labels = to_categorical(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_gPjfgKS1YY",
        "colab_type": "code",
        "outputId": "f7193b9d-9d53-41e6-a057-917f0f9c1dfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(one_hot_train_labels[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KpX2s1pTia6",
        "colab_type": "text"
      },
      "source": [
        "Now we are ready to apply Logostic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GAuyXqsThgV",
        "colab_type": "code",
        "outputId": "04377ac0-c71a-4adb-d42a-45244680cdcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
        "model.fit(x_train, train_labels)\n",
        "score = model.score(x_test, test_labels)\n",
        "print(\"Accuracy:\", score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7907390917186109\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}