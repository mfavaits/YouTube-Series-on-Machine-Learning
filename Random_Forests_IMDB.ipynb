{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Random Forests - IMDB.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mfavaits/YouTube-Series-on-Machine-Learning/blob/master/Random_Forests_IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Aoe_p0AaYhO",
        "colab_type": "text"
      },
      "source": [
        "The IMDB dataset actually comes packaged with keras and its allready tokenized, menaing the text is allready converted in a sequence of unique word indices. The IMDB dataset contains 50,000 movie reviews (25,000 for training and 25,000 for testing). Each set contains of 50% positive and 50% negative reviews (12,500 x 2). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de3MGD6O72TY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOhLB6msAMlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary=7500 # we will only use the 7500 most frequently used words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh2qyX4pAaqe",
        "colab_type": "text"
      },
      "source": [
        "Next code block is to fix a bug in keras. If bug would not exist this block would be just 1 line of code: (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocabulary)\n",
        "\n",
        "I suggest to just copy the fix and it does not really matter if you understand it or not\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shqvBezp-M7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save np.load\n",
        "np_load_old = np.load\n",
        "\n",
        "# modify the default parameters of np.load\n",
        "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
        "\n",
        "# call load_data with allow_pickle implicitly set to true\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocabulary)\n",
        "\n",
        "# restore np.load for future normal usage\n",
        "np.load = np_load_old"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG560c2_CYN6",
        "colab_type": "text"
      },
      "source": [
        "In the next line of code we will print the lists that contain sequences of words represented by a word index. If the text has not been converted to a sequence of indices we would need to add one pre-processing step using Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5qZFmD4Wx6l",
        "colab_type": "code",
        "outputId": "ed137034-2a62-4251-bd75-706656482364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "print(train_data[1]) # train_data is a list of word sequences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwgfRMAj2Hip",
        "colab_type": "text"
      },
      "source": [
        "Now we will vectorize the training and test data. Basically we will create a matrix where the rows are the reviews and where the columns represent the vocabulary (7500 columns). We will set a 1 in the correct column if the word of the review matches a word of the vocabulary. This means that matrix will be rather sparse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QELAzmk-03TM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_sequences(sequences, dimension=vocabulary):\n",
        "    results=np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence]=1\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-YwrSZEoyOs",
        "colab_type": "text"
      },
      "source": [
        "Now we apply the function to our training and test data as well as the labels. For the labels we use a different method. We simply use the asarray function to convert the list to an array and we assign the items in the array to float32 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRbl4N4d13te",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train=vectorize_sequences(train_data)\n",
        "x_test=vectorize_sequences(test_data)\n",
        "\n",
        "y_train=np.asarray(train_labels).astype('float32')\n",
        "y_test=np.asarray(test_labels).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTPkTl0jx__V",
        "colab_type": "text"
      },
      "source": [
        "Now we are ready to apply Random Forests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XWrRmruq1XR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model=RandomForestClassifier(n_estimators=100, random_state=42) #n_estimators provides the number of trees\n",
        "model.fit(x_train,y_train)\n",
        "y_pred=model.predict(x_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bg_DfHrg3z3",
        "colab_type": "code",
        "outputId": "d46a95d5-d20c-4c95-932a-eb969e0d8584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred=model.predict(x_test)\n",
        "confusion_matrix(y_test,y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[10675,  1825],\n",
              "       [ 2030, 10470]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOcO_Uw5hLac",
        "colab_type": "text"
      },
      "source": [
        "For 100 trees we have 2030 false negatives and 1825 false positives. Recall in our case is TP/(TP+FN) = 83%. Precission is TP/(TP+FP)=85%"
      ]
    }
  ]
}